# -*- coding: utf-8 -*-
"""KPF_KeyBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E4DCGrRU__vqH-cvBwDyMuxwHUN9fssG
"""

#  !curl -LJks -H "uname:$(uname -a)" https://bareun.ai/api/get -o bareun-linux.deb
# !ls

# !uname -a
# !dpkg -i bareun-linux.deb

# !curl -O https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.9.1.tar.gz
# !tar -C /opt/bareun -xzf libtensorflow-gpu-linux-x86_64-2.9.1.tar.gz

# Commented out IPython magic to ensure Python compatibility.
# %env BAREUN_ROOT="/opt/bareun"
# %env LD_LIBRARY_PATH="/opt/bareun/lib
# !BAREUN_ROOT="/opt/bareun" LD_LIBRARY_PATH="/opt/bareun/lib" nohup /opt/bareun/bin/bareun&

# !ps -ef | grep bareun

# !BAREUN_ROOT="/opt/bareun" LD_LIBRARY_PATH="/opt/bareun/lib" /opt/bareun/bin/bareun -reg koba-SBYM43A-VTYUGSI-XN4PLBY-46CHDKY

# pip install bareunpy

import sys
import bareunpy as brn
import google.protobuf.text_format as tf
from sentence_transformers import SentenceTransformer
import numpy as np
import itertools
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from bareunpy import Tagger
import pandas as pd
from tqdm import tqdm
import re
import argpaser


API_KEY = "YOUR API KEY" # 수정 필요 (Bareun 형태소 분석기 API)
tagger = brn.Tagger(API_KEY, "localhost", 5656)
res = tagger.tags(["안녕하세요. 정말 좋은 날씨네요."])
m = res.msg()
tf.PrintMessage(m, out=sys.stdout, as_utf8=True)

# !pip install sentence_transformers


model = SentenceTransformer('bongsoo/kpf-sbert-128d-v1')



def keyword_ext(text):

    tokenized_doc = tagger.pos(text)
    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])

    n_gram_range = (1, 1)

    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])
    candidates = count.get_feature_names_out()

    doc_embedding = model.encode([text])
    candidate_embeddings = model.encode(candidates)

    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=10, diversity=0.1)

def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):

    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트
    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)

    # 각 키워드들 간의 유사도
    word_similarity = cosine_similarity(candidate_embeddings)

    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.
    # 만약, 2번 문서가 가장 유사도가 높았다면
    # keywords_idx = [2]
    keywords_idx = [np.argmax(word_doc_similarity)]

    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들
    # 만약, 2번 문서가 가장 유사도가 높았다면
    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]
    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]

    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.
    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.
    for _ in range(top_n - 1):
        candidate_similarities = word_doc_similarity[candidates_idx, :]
        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)

        # MMR을 계산
        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)
        mmr_idx = candidates_idx[np.argmax(mmr)]

        # keywords & candidates를 업데이트
        keywords_idx.append(mmr_idx)
        candidates_idx.remove(mmr_idx)

    print(keywords_idx)

    return [words[idx] for idx in keywords_idx]

def main(filename):
    filepath = f'./data/{filename}.csv' # ex) ./data/news_2024-04-20.csv
    df = pd.read_csv(filename, encoding='utf8')
    kwdList = []
    for kw in tqdm(df['summary']):
        try:
            kwdList.append(keyword_ext(kw))
        except:
            kwdList.append('키워드_요약실패')

    df['summary'] = kwdList
    df.to_csv(filename, encoding='utf8', index=False)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='뉴스 기사 키워드 생성기")
    parser.add_argument('--filename', type=str, required=True, help='뉴스 본문, 요약문 수집된 파일 이름 입력')

    main(args.filename)
    

