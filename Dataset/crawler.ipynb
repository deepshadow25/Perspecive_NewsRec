{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(d):\n",
    "    d = d.lower()\n",
    "    d = re.sub(r'[a-z0-9\\-_.]{3,}@[a-z0-9\\-_.]{3,}(?:[.]?[a-z]{2})+', ' ', d)\n",
    "    d = re.sub(r'‘’ⓒ\\'\\\"“”…=□*◆:/_]', ' ', d)\n",
    "    d = re.sub(r'\\s+', ' ', d)\n",
    "    d = re.sub(r'^\\s|\\s$', '', d)\n",
    "    d = re.sub(r'[<*>a-z_=\"/()]', '', d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base URL 설정\n",
    "\n",
    "### 페이지수를 볼 수 있는 네이버 뉴스 주소 (일반 뉴스는 .../main/main.naver?... 주소로 이루어짐)\n",
    "'https://news.naver.com/main/list.naver?\n",
    "\n",
    "mode=LSD&\n",
    "\n",
    "mid=shm&\n",
    "\n",
    "sid1=100&   -- 카테고리\n",
    "\n",
    "date={date}& -- 상세카테고리\n",
    "\n",
    "page={page}' -- 페이지\n",
    "\n",
    "\n",
    "sid1 (카테고리) : 100(정치), 101(경제), 102(사회), 103(생활문화), 104(세계), 105(IT과학), 106(연예), 107(스포츠), 110(오피니언)\n",
    "\n",
    "sid2 (상세카테고리) : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://news.naver.com/main/list.naver?mode=LSD&mid=shm&sid1=100&date={date}&page={page}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예외 단어 처리 - 재사용하기 위해 컴파일로 미리 저장.\n",
    "exclude_keywords = re.compile(r'(속보|포토)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자주 쓸 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 콘텐츠 (본문) 수집기 함수 정의\n",
    "# 작동 안 됨 -> 다음 단계에서 본문 전문 수집. \n",
    "# def fetch_article_content(article_url):\n",
    "#     resp = get(article_url)\n",
    "#     article_dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "#     content_tag = article_dom.select_one('div#articleBodyContents')\n",
    "#     if content_tag:\n",
    "#         return content_tag.get_text(strip=True)\n",
    "#     return ''\n",
    "\n",
    "# 기사 가져오는 함수\n",
    "def fetch_articles(url):\n",
    "    resp = get(url)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    articles = dom.select('.content ul.type06_headline li')\n",
    "    article_data_list = []\n",
    "\n",
    "    # DOM select로 얻어낸 html 반복문 실행하여 데이터 추출\n",
    "    for article in articles:\n",
    "        title_tag = article.find('dt', class_=None)\n",
    "        title_text = title_tag.get_text(strip=True)\n",
    "       \n",
    "        # 제목내 예외단어 발견시 추출 건너뛰기 (넘어가기)\n",
    "        if exclude_keywords.search(title_text):\n",
    "            continue\n",
    "    \n",
    "        # 기사 링크 태그, 주소, 요약 태그, 기자 태그\n",
    "        link_tag = title_tag.find('a')\n",
    "        article_url = link_tag['href']\n",
    "        summary_tag = article.find('span', class_='lede')\n",
    "        source_tag = article.find('span', class_='writing')\n",
    "        \n",
    "        # 기사 콘텐츠 (본문) 수집기\n",
    "        # content = fetch_article_content(article_url)\n",
    "\n",
    "        # 기사별 메타데이터 딕셔너리 생성\n",
    "        article_dict = {\n",
    "            'title': title_text,\n",
    "            'link': article_url,\n",
    "            'summary': summary_tag.get_text(strip=True) if summary_tag else '',\n",
    "            'source': source_tag.get_text(strip=True) if source_tag else '',\n",
    "            # 'content': content\n",
    "        }\n",
    "        \n",
    "        # 여러 기사에 대해 메타데이터 작성\n",
    "        article_data_list.append(article_dict)\n",
    "\n",
    "    return article_data_list, dom\n",
    "\n",
    "# 다음 페이지 넘어가기 : 최대 10페이지 전까지 정의\n",
    "def get_next_page(dom, current_page):\n",
    "    if current_page < 10:\n",
    "        next_page_tag = dom.select_one(f'.paging a[href*=\"page={current_page + 1}\"]')\n",
    "        if next_page_tag:\n",
    "            return urljoin(base_url, next_page_tag['href'])\n",
    "    return None\n",
    "\n",
    "# 일자별 크롤링 함수\n",
    "def generate_dates(start_date, end_date):\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        yield current_date\n",
    "        current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기사 메타데이터 (제목, 링크, 기자, 요약 등) 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 수집 시작 / 종료 일자 (ex 2022-05-10: 2022년 5월 10일자)\n",
    "start_date = datetime.strptime('2024-04-23', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2024-04-24', '%Y-%m-%d')\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "for single_date in tqdm(generate_dates(start_date, end_date)):\n",
    "    date_str = single_date.strftime('%Y%m%d')\n",
    "    page = 1\n",
    "    url = base_url.format(date=date_str, page=page)\n",
    "    \n",
    "    while url and page <= 10:\n",
    "        article_data_list, dom = fetch_articles(url)\n",
    "        all_articles.extend(article_data_list)\n",
    "        url = get_next_page(dom, page)\n",
    "        page += 1\n",
    "        time.sleep(2)  # 서버에 부담을 주지 않도록 잠시 대기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기사 본문 전문 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 생성\n",
    "df1 = pd.DataFrame(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 링크 리스트(시리즈) 뽑아내기\n",
    "list(df1['link']) # type : list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 본문 (전문), 기자 정보 추출하기\n",
    "\n",
    "article_data_list = []\n",
    "\n",
    "def fetch_article_data(article_url):\n",
    "    resp = get(article_url)\n",
    "    article_dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    \n",
    "    # 기사 제목 추출\n",
    "    # title_tag = article_dom.select_one('h2#title_area')\n",
    "    # title = title_tag.get_text(strip=True) if title_tag else ''\n",
    "\n",
    "    # 기사 본문 추출\n",
    "    content_tag = article_dom.select_one('article')\n",
    "    content = preprocessing(content_tag.get_text(strip=True)) if content_tag else ''\n",
    "\n",
    "    # 언론사 정보 추출\n",
    "    # source_tag = article_dom.select_one('meta[property=\"og:article:author\"]')\n",
    "    # source = source_tag['content'] if source_tag else ''\n",
    "    \n",
    "    # 기자 정보 추출\n",
    "    reporter_tag = article_dom.select_one('div.byline span')\n",
    "    reporter = reporter_tag.get_text(strip=True) if reporter_tag else ''\n",
    "\n",
    "    article_data = {\n",
    "        # 'title': title,\n",
    "        'link': article_url,\n",
    "        'content': content,\n",
    "        # 'source': source,\n",
    "        'reporter': reporter\n",
    "    }\n",
    "\n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [02:09<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for url in tqdm(list(df['link'])):\n",
    "    article_data = fetch_article_data(url)\n",
    "    article_data_list.append(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(article_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터프레임 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1, df2)\n",
    "\n",
    "# 데이터프레임 열 순서 바꾸기\n",
    "df = df[['source', 'reporter', 'title', 'summary', 'content', 'link']]\n",
    "\n",
    "# 중복 제거\n",
    "df.drop_duplicates(subset=None, keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "# 열 이름 변경\n",
    "df.columns = ['언론사', '기자', '제목', '기사요약', '기사전문', '기사링크']\n",
    "\n",
    "# 데이터프레임 형태 확인\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일로 저장되었습니다: 뉴스기사_2024-04-23_2024-04-24.csv\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일로 저장\n",
    "csv_filename = f'뉴스기사_{str(start_date)[:10]}_{str(end_date)[:10]}.csv'\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일로 저장되었습니다: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ku-sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
